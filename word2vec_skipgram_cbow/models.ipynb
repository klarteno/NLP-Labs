{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EHvMvS812UI2"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","import numpy as np"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\", force_remount=True)\n","%cd '/content/drive/My Drive/Colab Notebooks/Labs/NLP/word2vec'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F24-9V8v3PCs","executionInfo":{"status":"ok","timestamp":1669850891756,"user_tz":-60,"elapsed":21360,"user":{"displayName":"julyan n","userId":"13617636357881634761"}},"outputId":"d401571d-1229-4fa0-8c96-b4d58ac32af3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/Labs/NLP/word2vec\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ten5OBbi2UI4"},"outputs":[],"source":["TABLE_SIZE = 1e8\n","\n","\n","def create_sample_table(word_count):\n","    \"\"\"Create negative sample table for vocabulary, words with\n","    higher frequency will have higher occurrences in table.\n","    \"\"\"\n","    table = []\n","    frequency = np.power(np.array(word_count), 0.75)\n","    sum_frequency = sum(frequency)\n","    ratio = frequency / sum_frequency\n","    count = np.round(ratio * TABLE_SIZE)\n","    for word_idx, c in enumerate(count):\n","        table += [word_idx] * int(c)\n","    return np.array(table)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IB6CEMzb2UI5"},"outputs":[],"source":["class SkipGramModel(torch.nn.Module):\n","    \"\"\"Center word as input, context words as target.\n","    Objective is to maximize the score of map from input to target.\n","    \"\"\"\n","\n","    def __init__(\n","        self, device, vocabulary_size, embedding_dim, neg_num=0, word_count=[]\n","    ):\n","        super(SkipGramModel, self).__init__()\n","        self.device = device\n","        self.neg_num = neg_num\n","        self.embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n","        initrange = 0.5 / embedding_dim\n","        self.embeddings.weight.data.uniform_(-initrange, initrange)\n","        if self.neg_num > 0:\n","            self.table = create_sample_table(word_count)\n","\n","    def forward(self, centers, contexts):\n","        batch_size = len(centers)\n","        u_embeds = self.embeddings(centers).view(batch_size, 1, -1)\n","        v_embeds = self.embeddings(contexts).view(batch_size, 1, -1)\n","        score = torch.bmm(u_embeds, v_embeds.transpose(1, 2)).squeeze()\n","        loss = F.logsigmoid(score).squeeze()\n","\n","        if self.neg_num > 0:\n","            neg_contexts = torch.LongTensor(\n","                np.random.choice(self.table, size=(batch_size, self.neg_num))\n","            ).to(self.device)\n","            neg_v_embeds = self.embeddings(neg_contexts)\n","            neg_score = torch.bmm(u_embeds, neg_v_embeds.transpose(1, 2)).squeeze()\n","            neg_score = torch.sum(neg_score, dim=1)\n","            neg_score = F.logsigmoid(-1 * neg_score).squeeze()\n","            loss += neg_score\n","        return -1 * loss.sum()\n","\n","    def get_embeddings(self):\n","        return self.embeddings.weight.data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIft-01y2UI6"},"outputs":[],"source":["class CBOWModel(torch.nn.Module):\n","    \"\"\"Context words as input, returns possiblity distribution\n","    prediction of center word (target).\n","    \"\"\"\n","\n","    def __init__(self, device, vocabulary_size, embedding_dim):\n","        super(CBOWModel, self).__init__()\n","        self.device = device\n","        self.embeddings = torch.nn.Embedding(vocabulary_size, embedding_dim)\n","        initrange = 0.5 / embedding_dim\n","        self.embeddings.weight.data.uniform_(-initrange, initrange)\n","        self.linear = torch.nn.Linear(embedding_dim, vocabulary_size)\n","\n","    def forward(self, contexts):\n","        # input\n","        embeds = self.embeddings(contexts)\n","        # projection\n","        add_embeds = torch.sum(embeds, dim=1)\n","        # output\n","        out = self.linear(add_embeds)\n","        log_probs = F.log_softmax(out, dim=1)\n","        return log_probs\n","\n","    def get_embeddings(self):\n","        return self.embeddings.weight.data"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('pytorch_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"201b6068cbed112f118c29a6393440d4ce4dc02105f31305c61d838eb972bb93"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}